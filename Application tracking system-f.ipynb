{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9845387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-docx\n",
    "#pip install openai\n",
    "#pip install --upgrade typing-extensions\n",
    "#pip install PyPDF2\n",
    "#pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f633e3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\rishi\\\\Capstone'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0d34438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rishiikeshwer Brindha Sivakumar  \\nrbrindha@gmu.edu  | +1 (571) -587-6358 | www.linkedin.com/in/rishiikeshwer  | \\nhttps://github.com/Rishi0498/Project_Portfolio.git  |Fairfax, Virginia, 22031  \\nEDUCATION  \\n \\nMaster of Science (Data  Analytics  Engineering)                          GPA:  4.0/4.0  \\nGeorge Mason  University , Virginia                       Aug 2022 –May 2024  \\nCoursework: Applied Statistics and Visualization, Big Data  to Information,  Big Data Essentials, Principles of Data Ma nagement and \\nMining, Introduction to Natural Language Processing , Advanced Data M ining for Business Application  \\nBachelor of Technology  (Aerospace  Engineering)                           GPA:  3.1/4.0  \\nSASTRA Deemed  University,  India                          Jun 2016 –Jun 2020  \\nSKILLS  \\n \\n   Programming: Python, R, C++, C, HTML, PySpark            Databases:  SQL, NoSQL, Hadoop, MongoDB, PostgreSQL                                                      \\n Excel Statistics: Hypothesis Testing, Inferential Statistics, A/B testing  \\n      Machine Learning: EDA, Regression, Classification, Clustering, Time Series Forecasting, NLP, PCA.        \\n   Tools and Platforms: AWS,  Power BI,  OpenVino, Tensorflow,  Kubeflow, Kafka, Advanced Excel (Pivot Tabl es / VLOOKUP / Macr os), ETL, \\nKubernetes, RedHat OpenShift , JIRA , Shiny . \\n Predictive Modeling Packages/Tools:  Keras, Pytorch, Pandas , Numpy, Analytical Solver, Sci kit, Matplotlib , Plotly, Docker, GIT   \\nEXPERIENCE  \\n \\n   George Mason University                                                                                                                                                             Fairfax, Virginia  \\n   Graduate Teaching Assistant   Jan 2024 –Present  \\n• Co-ordinated  and assessed  a graduate -level  course on Big Data Essentials  for 2 cohorts , overseeing the comprehensive delivery \\nof key concepts.  \\n• Led lab sessions on Python u tilizing the  Databricks platform for  80 students , providing  guid ance  on data science projects.  \\n    Quant Gov  Limited           Arlington, Virginia  \\n    Policy Analytics Research Intern           Aug 2023 –Dec 2023  \\n• Spearheaded web scraping to extract FOMC statements  and applied advanced text pre -processing, including tokenization, \\nlemmatization, stop words removal, NER, and analysis of lexical semantics to enhance sentiment analysis accuracy.  \\n• Customized pre -trained BERT models to gauge appropriate sentiments of the statements, resulting in a significant 12% increase \\nin accuracy compared to existing models.  \\n• Developed a user -friendly Streamlit app utilizing advanced techniques to predict sentiment in FOMC statements;  reduced \\nanalysis time by 40% and enhanced decision -making accuracy.  \\nWipro  Limited           Bangalore,  India  \\nData  Scientist           Sep 2020 –Oct 2021  \\n• Established  an end -to-end Covid -19 protocol interface using Python  scripting  language utilizing real -time m edia feed and DL -\\nstreamer  in a CI/CD pipeline with  Intel’s OpenVino Toolkit, resulting in a 40% reduction in response time.  \\n• Designed and implemented a Supervised model for Facemask Detection, creating an ETL pipeline using AWS Glue that increased \\nthe model’s accuracy by 15% (from 81% to 96%) and decreased the inference time by 25%.  \\n• Spearheaded  a team of 5+ deep learning developers in an agile project on the ‘cnvrg’ platform, improving  the team efficiency by \\n20% in building models with superior  results  and performed ad -hoc analysis.  \\n• Performed Unit, System , and A/B testing, identifying & reporting discrepancies in ML models during development,  and monitored \\ndata quality  resulting in  a 10% increase in overall model accuracy.  \\nTechnology  Business  Incubator         Thanjavur,  India  \\nData Science Intern (Deep  Learning  Applications)         Oct 2019 – May  2020  \\n• Developed a Convolutional Neural Network with 98% training accuracy for real -time detection of diseased leaves, cross -validated \\nfor consistency. Created a recommendation system for pesticides/herbicides, leading to a 25% r eduction in crop loss , a 15% \\nincrease in crop yield, and improved adoption by farmers.  \\nPROJECT PORTFOLIO  \\n \\nAWS - Integrated Data Engineering Pipeline (YouTube Analytics)                              Jan 2024 – Feb 2024  \\n• Built  a highly secure, scalable, and cost -efficient AWS -based data  pipeline  to analyze the YouTube dataset. The architecture \\nutilized AWS services, such as S3, Glue for ETL, Lambda and Athena for  compute and  ad-hoc quer ying, and CloudWatch . This  \\nfacilitated data visualization with Power BI, delivering key insights into viewer behavior and content efficacy.  \\nMultilingual Tweet Intimacy Analysis                                    Aug 2023 – Dec 2023  \\n• Implemented  machine learning models for the classification of tweets based on intimacy levels on a scale of 5.  Conducted  \\ncomparative analysis with custom pre -trained BERT models and achieved an accuracy rate of 81% through zero -shot testing  and \\nanalyzed user behavior . \\nStarlink Orbital Predictions                                      Aug 2023 – Dec 2023  \\n• Applied  K-means Clustering and PCA to Celestrak TLE orbital data to identify Non -Starlink Satellites with similar orbital parameters to \\nStarli nk Satellites and utilized SGP4 algorithm to determine  a ratio of potential conjunction of Starlink with non -Starlink satellite s. \\n PUBLICATIONS  \\n• Mobile Application for Automated Leaf Disease Detection of Real -Time  Data set using Convolution Neural \\nNetworks, Journal of Computer Science, Vol  16 DOI:   https://doi.org/10.3844/jcssp.2020.158.166   \\n• Precise prediction of anticancer drug efficacy using multi target regression and support vector regression  \\nanalysis. Computation Methods Programs Biomed. 2022. https://pubmed.ncbi.nlm.nih.gov/35914385/   \\n',\n",
       " 'Rishiikeshwer Brindha Sivakumar  \\nrbrindha@gmu.edu  | +1 (571) -587-6358 | www.linkedin.com/in/rishiikeshwer  | \\nhttps://github.com/Rishi0498/Project_Portfolio.git  |Fairfax, Virginia, 22031  \\nEDUCATION  \\n \\nMaster of Science (Data  Analytics  Engineering)                          GPA:  4.0/4.0  \\nGeorge Mason  University , Virginia                       Aug 2022 –May 2024  \\nCoursework: Applied Statistics and Visualization, Big Data  to Information,  Big Data Essentials, Principles of Data Ma nagement and \\nMining, Introduction to Natural Language Processing , Advanced Data M ining for Business Application  \\nBachelor of Technology  (Aerospace  Engineering)                           GPA:  3.1/4.0  \\nSASTRA Deemed  University,  India                          Jun 2016 –Jun 2020  \\nSKILLS  \\n \\n   Programming: Python, R, C++, C, HTML, PySpark            Databases:  SQL, NoSQL, Hadoop, MongoDB, PostgreSQL                                                      \\n Excel Statistics: Hypothesis Testing, Inferential Statistics, A/B testing  \\n      Machine Learning: EDA, Regression, Classification, Clustering, Time Series Forecasting, NLP, PCA.        \\n   Tools and Platforms: AWS,  Power BI,  OpenVino, Tensorflow,  Kubeflow, Kafka, Advanced Excel (Pivot Tabl es / VLOOKUP / Macr os), ETL, \\nKubernetes, RedHat OpenShift , JIRA , Shiny . \\n Predictive Modeling Packages/Tools:  Keras, Pytorch, Pandas , Numpy, Analytical Solver, Sci kit, Matplotlib , Plotly, Docker, GIT   \\nEXPERIENCE  \\n \\n   George Mason University                                                                                                                                                             Fairfax, Virginia  \\n   Graduate Teaching Assistant   Jan 2024 –Present  \\n• Co-ordinated  and assessed  a graduate -level  course on Big Data Essentials  for 2 cohorts , overseeing the comprehensive delivery \\nof key concepts.  \\n• Led lab sessions on Python u tilizing the  Databricks platform for  80 students , providing  guid ance  on data science projects.  \\n    Quant Gov  Limited           Arlington, Virginia  \\n    Policy Analytics Research Intern           Aug 2023 –Dec 2023  \\n• Spearheaded web scraping to extract FOMC statements  and applied advanced text pre -processing, including tokenization, \\nlemmatization, stop words removal, NER, and analysis of lexical semantics to enhance sentiment analysis accuracy.  \\n• Customized pre -trained BERT models to gauge appropriate sentiments of the statements, resulting in a significant 12% increase \\nin accuracy compared to existing models.  \\n• Developed a user -friendly Streamlit app utilizing advanced techniques to predict sentiment in FOMC statements;  reduced \\nanalysis time by 40% and enhanced decision -making accuracy.  \\nWipro  Limited           Bangalore,  India  \\nData  Scientist           Sep 2020 –Oct 2021  \\n• Established  an end -to-end Covid -19 protocol interface using Python  scripting  language utilizing real -time m edia feed and DL -\\nstreamer  in a CI/CD pipeline with  Intel’s OpenVino Toolkit, resulting in a 40% reduction in response time.  \\n• Designed and implemented a Supervised model for Facemask Detection, creating an ETL pipeline using AWS Glue that increased \\nthe model’s accuracy by 15% (from 81% to 96%) and decreased the inference time by 25%.  \\n• Spearheaded  a team of 5+ deep learning developers in an agile project on the ‘cnvrg’ platform, improving  the team efficiency by \\n20% in building models with superior  results  and performed ad -hoc analysis.  \\n• Performed Unit, System , and A/B testing, identifying & reporting discrepancies in ML models during development,  and monitored \\ndata quality  resulting in  a 10% increase in overall model accuracy.  \\nTechnology  Business  Incubator         Thanjavur,  India  \\nData Science Intern (Deep  Learning  Applications)         Oct 2019 – May  2020  \\n• Developed a Convolutional Neural Network with 98% training accuracy for real -time detection of diseased leaves, cross -validated \\nfor consistency. Created a recommendation system for pesticides/herbicides, leading to a 25% r eduction in crop loss , a 15% \\nincrease in crop yield, and improved adoption by farmers.  \\nPROJECT PORTFOLIO  \\n \\nAWS - Integrated Data Engineering Pipeline (YouTube Analytics)                              Jan 2024 – Feb 2024  \\n• Built  a highly secure, scalable, and cost -efficient AWS -based data  pipeline  to analyze the YouTube dataset. The architecture \\nutilized AWS services, such as S3, Glue for ETL, Lambda and Athena for  compute and  ad-hoc quer ying, and CloudWatch . This  \\nfacilitated data visualization with Power BI, delivering key insights into viewer behavior and content efficacy.  \\nMultilingual Tweet Intimacy Analysis                                    Aug 2023 – Dec 2023  \\n• Implemented  machine learning models for the classification of tweets based on intimacy levels on a scale of 5.  Conducted  \\ncomparative analysis with custom pre -trained BERT models and achieved an accuracy rate of 81% through zero -shot testing  and \\nanalyzed user behavior . \\nStarlink Orbital Predictions                                      Aug 2023 – Dec 2023  \\n• Applied  K-means Clustering and PCA to Celestrak TLE orbital data to identify Non -Starlink Satellites with similar orbital parameters to \\nStarli nk Satellites and utilized SGP4 algorithm to determine  a ratio of potential conjunction of Starlink with non -Starlink satellite s. \\n PUBLICATIONS  \\n• Mobile Application for Automated Leaf Disease Detection of Real -Time  Data set using Convolution Neural \\nNetworks, Journal of Computer Science, Vol  16 DOI:   https://doi.org/10.3844/jcssp.2020.158.166   \\n• Precise prediction of anticancer drug efficacy using multi target regression and support vector regression  \\nanalysis. Computation Methods Programs Biomed. 2022. https://pubmed.ncbi.nlm.nih.gov/35914385/   \\n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import glob\n",
    "from docx import Document\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "\n",
    "#extracting file names of all the files in the DB\n",
    "list_of_files = list()\n",
    "files_grabbed = list()\n",
    "list_file_types = ('test folder\\\\*.docx','test folder\\\\*.pdf')\n",
    "for files in list_file_types:\n",
    "    files_grabbed.extend(glob.glob(files))\n",
    "\n",
    "for file_name in files_grabbed:\n",
    "  FI = open(file_name, 'r')\n",
    "  list_of_files.append(file_name)\n",
    "  FI.close()\n",
    "\n",
    "\n",
    "#iterating through all the files and \n",
    "resume_list = list()\n",
    "for file in list_of_files:\n",
    "    try:\n",
    "        if file.lower().endswith('.pdf'):\n",
    "         extracted_text = extract_text_from_pdf(file)\n",
    "         resume_list.append(extracted_text)\n",
    "        \n",
    "        elif file.lower().endswith('.docx'):\n",
    "         extracted_text = extract_text_from_docx(file)\n",
    "        resume_list.append(extracted_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "list_of_files\n",
    "resume_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8854d0d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT Response:\n",
      "```python\n",
      "{\n",
      "    \"name\": \"Rishiikeshwer Brindha Sivakumar\",\n",
      "    \"key_skills\": [\n",
      "        \"Python\", \"R\", \"C++\", \"C\", \"HTML\", \"PySpark\",\n",
      "        \"SQL\", \"NoSQL\", \"Hadoop\", \"MongoDB\", \"PostgreSQL\",\n",
      "        \"Hypothesis Testing\", \"Inferential Statistics\", \"A/B testing\",\n",
      "        \"EDA\", \"Regression\", \"Classification\", \"Clustering\", \"Time Series Forecasting\", \"NLP\", \"PCA\",\n",
      "        \"AWS\", \"Power BI\", \"OpenVino\", \"Tensorflow\", \"Kubeflow\", \"Kafka\", \"Advanced Excel\", \"ETL\",\n",
      "        \"Kubernetes\", \"RedHat OpenShift\", \"JIRA\", \"Shiny\",\n",
      "        \"Keras\", \"Pytorch\", \"Pandas\", \"Numpy\", \"Analytical Solver\", \"Scikit\", \"Matplotlib\", \"Plotly\", \"Docker\", \"GIT\"\n",
      "    ]\n",
      "}\n",
      "```\n",
      "ChatGPT Response:\n",
      "```python\n",
      "{\n",
      "    \"name\": \"Rishiikeshwer Brindha Sivakumar\",\n",
      "    \"key_skills\": [\n",
      "        \"Python\", \"R\", \"C++\", \"C\", \"HTML\", \"PySpark\",\n",
      "        \"SQL\", \"NoSQL\", \"Hadoop\", \"MongoDB\", \"PostgreSQL\",\n",
      "        \"Hypothesis Testing\", \"Inferential Statistics\", \"A/B testing\",\n",
      "        \"EDA\", \"Regression\", \"Classification\", \"Clustering\", \"Time Series Forecasting\", \"NLP\", \"PCA\",\n",
      "        \"AWS\", \"Power BI\", \"OpenVino\", \"Tensorflow\", \"Kubeflow\", \"Kafka\", \"Advanced Excel\", \"ETL\",\n",
      "        \"Kubernetes\", \"RedHat OpenShift\", \"JIRA\", \"Shiny\",\n",
      "        \"Keras\", \"Pytorch\", \"Pandas\", \"Numpy\", \"Analytical Solver\", \"Scikit\", \"Matplotlib\", \"Plotly\", \"Docker\", \"GIT\"\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['```python\\n{\\n    \"name\": \"Rishiikeshwer Brindha Sivakumar\",\\n    \"key_skills\": [\\n        \"Python\", \"R\", \"C++\", \"C\", \"HTML\", \"PySpark\",\\n        \"SQL\", \"NoSQL\", \"Hadoop\", \"MongoDB\", \"PostgreSQL\",\\n        \"Hypothesis Testing\", \"Inferential Statistics\", \"A/B testing\",\\n        \"EDA\", \"Regression\", \"Classification\", \"Clustering\", \"Time Series Forecasting\", \"NLP\", \"PCA\",\\n        \"AWS\", \"Power BI\", \"OpenVino\", \"Tensorflow\", \"Kubeflow\", \"Kafka\", \"Advanced Excel\", \"ETL\",\\n        \"Kubernetes\", \"RedHat OpenShift\", \"JIRA\", \"Shiny\",\\n        \"Keras\", \"Pytorch\", \"Pandas\", \"Numpy\", \"Analytical Solver\", \"Scikit\", \"Matplotlib\", \"Plotly\", \"Docker\", \"GIT\"\\n    ]\\n}\\n```',\n",
       " '```python\\n{\\n    \"name\": \"Rishiikeshwer Brindha Sivakumar\",\\n    \"key_skills\": [\\n        \"Python\", \"R\", \"C++\", \"C\", \"HTML\", \"PySpark\",\\n        \"SQL\", \"NoSQL\", \"Hadoop\", \"MongoDB\", \"PostgreSQL\",\\n        \"Hypothesis Testing\", \"Inferential Statistics\", \"A/B testing\",\\n        \"EDA\", \"Regression\", \"Classification\", \"Clustering\", \"Time Series Forecasting\", \"NLP\", \"PCA\",\\n        \"AWS\", \"Power BI\", \"OpenVino\", \"Tensorflow\", \"Kubeflow\", \"Kafka\", \"Advanced Excel\", \"ETL\",\\n        \"Kubernetes\", \"RedHat OpenShift\", \"JIRA\", \"Shiny\",\\n        \"Keras\", \"Pytorch\", \"Pandas\", \"Numpy\", \"Analytical Solver\", \"Scikit\", \"Matplotlib\", \"Plotly\", \"Docker\", \"GIT\"\\n    ]\\n}\\n```']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.api_key = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' # OpenAI API key\n",
    "all_responses = [] \n",
    "def generate_response(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "         messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        max_tokens=4000,  # Adjust as needed\n",
    "        temperature=0,  # Adjust as needed (higher values for more randomness)\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "custom = \"From the above resumes, extract name of the candidate and all the key skills as keywords and return as a python dictionary\"\n",
    "\n",
    "for item_in_list in resume_list:\n",
    "    prompt = custom + '\\n'+ item_in_list # here 'extracted_text' contains the text we want to send to ChatGPT\n",
    "    try:\n",
    "        chatgpt_response = generate_response(prompt)\n",
    "        print(\"ChatGPT Response:\")\n",
    "        print(chatgpt_response)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    all_responses.append(chatgpt_response)   \n",
    "\n",
    "all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65527cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['```python\\n{\\n    \"name\": \"Rishiikeshwer Brindha Sivakumar\",\\n    \"key_skills\": [\\n        \"Python\", \"R\", \"C++\", \"C\", \"HTML\", \"PySpark\",\\n        \"SQL\", \"NoSQL\", \"Hadoop\", \"MongoDB\", \"PostgreSQL\",\\n        \"Hypothesis Testing\", \"Inferential Statistics\", \"A/B testing\",\\n        \"EDA\", \"Regression\", \"Classification\", \"Clustering\", \"Time Series Forecasting\", \"NLP\", \"PCA\",\\n        \"AWS\", \"Power BI\", \"OpenVino\", \"Tensorflow\", \"Kubeflow\", \"Kafka\", \"Advanced Excel\", \"ETL\",\\n        \"Kubernetes\", \"RedHat OpenShift\", \"JIRA\", \"Shiny\",\\n        \"Keras\", \"Pytorch\", \"Pandas\", \"Numpy\", \"Analytical Solver\", \"Scikit\", \"Matplotlib\", \"Plotly\", \"Docker\", \"GIT\"\\n    ]\\n}\\n```',\n",
       " '```python\\n{\\n    \"name\": \"Rishiikeshwer Brindha Sivakumar\",\\n    \"key_skills\": [\\n        \"Python\", \"R\", \"C++\", \"C\", \"HTML\", \"PySpark\",\\n        \"SQL\", \"NoSQL\", \"Hadoop\", \"MongoDB\", \"PostgreSQL\",\\n        \"Hypothesis Testing\", \"Inferential Statistics\", \"A/B testing\",\\n        \"EDA\", \"Regression\", \"Classification\", \"Clustering\", \"Time Series Forecasting\", \"NLP\", \"PCA\",\\n        \"AWS\", \"Power BI\", \"OpenVino\", \"Tensorflow\", \"Kubeflow\", \"Kafka\", \"Advanced Excel\", \"ETL\",\\n        \"Kubernetes\", \"RedHat OpenShift\", \"JIRA\", \"Shiny\",\\n        \"Keras\", \"Pytorch\", \"Pandas\", \"Numpy\", \"Analytical Solver\", \"Scikit\", \"Matplotlib\", \"Plotly\", \"Docker\", \"GIT\"\\n    ]\\n}\\n```']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504eedd0",
   "metadata": {},
   "source": [
    "tasks at hand\n",
    "\n",
    "0. create a dummy key value pair, for resumes and the text extracted for matching ( for easy extraction post processing)\n",
    "1. create a prompt entry for descriptions\n",
    "2. create automation for all the resumes and put them in an array (**solved**)\n",
    "3. send the resumes to chatgpt and ask for a ranking and see if it is consistent *enough* to proceed\n",
    "4. export the resumes by id- tagging\n",
    "5. try to shorten the resume's because there is a token limit for our API/ and chatGPT( example above has 12 page resume)(**getting an error here**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91eea36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(all_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fb91336",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses_split=[]\n",
    "for i in all_responses:\n",
    "    each_iteration = i.split('\"')\n",
    "    all_responses_split.append(each_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40ea813b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['```python\\n{\\n    ', 'name', ': ', 'Rishiikeshwer Brindha Sivakumar', ',\\n    ', 'key_skills', ': [\\n        ', 'Python', ', ', 'R', ', ', 'C++', ', ', 'C', ', ', 'HTML', ', ', 'PySpark', ',\\n        ', 'SQL', ', ', 'NoSQL', ', ', 'Hadoop', ', ', 'MongoDB', ', ', 'PostgreSQL', ',\\n        ', 'Hypothesis Testing', ', ', 'Inferential Statistics', ', ', 'A/B testing', ',\\n        ', 'EDA', ', ', 'Regression', ', ', 'Classification', ', ', 'Clustering', ', ', 'Time Series Forecasting', ', ', 'NLP', ', ', 'PCA', ',\\n        ', 'AWS', ', ', 'Power BI', ', ', 'OpenVino', ', ', 'Tensorflow', ', ', 'Kubeflow', ', ', 'Kafka', ', ', 'Advanced Excel', ', ', 'ETL', ',\\n        ', 'Kubernetes', ', ', 'RedHat OpenShift', ', ', 'JIRA', ', ', 'Shiny', ',\\n        ', 'Keras', ', ', 'Pytorch', ', ', 'Pandas', ', ', 'Numpy', ', ', 'Analytical Solver', ', ', 'Scikit', ', ', 'Matplotlib', ', ', 'Plotly', ', ', 'Docker', ', ', 'GIT', '\\n    ]\\n}\\n```'], ['```python\\n{\\n    ', 'name', ': ', 'Rishiikeshwer Brindha Sivakumar', ',\\n    ', 'key_skills', ': [\\n        ', 'Python', ', ', 'R', ', ', 'C++', ', ', 'C', ', ', 'HTML', ', ', 'PySpark', ',\\n        ', 'SQL', ', ', 'NoSQL', ', ', 'Hadoop', ', ', 'MongoDB', ', ', 'PostgreSQL', ',\\n        ', 'Hypothesis Testing', ', ', 'Inferential Statistics', ', ', 'A/B testing', ',\\n        ', 'EDA', ', ', 'Regression', ', ', 'Classification', ', ', 'Clustering', ', ', 'Time Series Forecasting', ', ', 'NLP', ', ', 'PCA', ',\\n        ', 'AWS', ', ', 'Power BI', ', ', 'OpenVino', ', ', 'Tensorflow', ', ', 'Kubeflow', ', ', 'Kafka', ', ', 'Advanced Excel', ', ', 'ETL', ',\\n        ', 'Kubernetes', ', ', 'RedHat OpenShift', ', ', 'JIRA', ', ', 'Shiny', ',\\n        ', 'Keras', ', ', 'Pytorch', ', ', 'Pandas', ', ', 'Numpy', ', ', 'Analytical Solver', ', ', 'Scikit', ', ', 'Matplotlib', ', ', 'Plotly', ', ', 'Docker', ', ', 'GIT', '\\n    ]\\n}\\n```']]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(all_responses_split)\n",
    "print(type(all_responses_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0e963fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "new_list = []\n",
    "\n",
    "for i in all_responses_split:\n",
    "    temp_list =[]\n",
    "    for j in i:\n",
    "        remove_special_chars = re.sub('[^A-Za-z0-9]+',' ', j)\n",
    "        temp_list.append(remove_special_chars)\n",
    "    new_list.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "198c44a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' python ',\n",
       "  'name',\n",
       "  ' ',\n",
       "  'Rishiikeshwer Brindha Sivakumar',\n",
       "  ' ',\n",
       "  'key skills',\n",
       "  ' ',\n",
       "  'Python',\n",
       "  ' ',\n",
       "  'R',\n",
       "  ' ',\n",
       "  'C ',\n",
       "  ' ',\n",
       "  'C',\n",
       "  ' ',\n",
       "  'HTML',\n",
       "  ' ',\n",
       "  'PySpark',\n",
       "  ' ',\n",
       "  'SQL',\n",
       "  ' ',\n",
       "  'NoSQL',\n",
       "  ' ',\n",
       "  'Hadoop',\n",
       "  ' ',\n",
       "  'MongoDB',\n",
       "  ' ',\n",
       "  'PostgreSQL',\n",
       "  ' ',\n",
       "  'Hypothesis Testing',\n",
       "  ' ',\n",
       "  'Inferential Statistics',\n",
       "  ' ',\n",
       "  'A B testing',\n",
       "  ' ',\n",
       "  'EDA',\n",
       "  ' ',\n",
       "  'Regression',\n",
       "  ' ',\n",
       "  'Classification',\n",
       "  ' ',\n",
       "  'Clustering',\n",
       "  ' ',\n",
       "  'Time Series Forecasting',\n",
       "  ' ',\n",
       "  'NLP',\n",
       "  ' ',\n",
       "  'PCA',\n",
       "  ' ',\n",
       "  'AWS',\n",
       "  ' ',\n",
       "  'Power BI',\n",
       "  ' ',\n",
       "  'OpenVino',\n",
       "  ' ',\n",
       "  'Tensorflow',\n",
       "  ' ',\n",
       "  'Kubeflow',\n",
       "  ' ',\n",
       "  'Kafka',\n",
       "  ' ',\n",
       "  'Advanced Excel',\n",
       "  ' ',\n",
       "  'ETL',\n",
       "  ' ',\n",
       "  'Kubernetes',\n",
       "  ' ',\n",
       "  'RedHat OpenShift',\n",
       "  ' ',\n",
       "  'JIRA',\n",
       "  ' ',\n",
       "  'Shiny',\n",
       "  ' ',\n",
       "  'Keras',\n",
       "  ' ',\n",
       "  'Pytorch',\n",
       "  ' ',\n",
       "  'Pandas',\n",
       "  ' ',\n",
       "  'Numpy',\n",
       "  ' ',\n",
       "  'Analytical Solver',\n",
       "  ' ',\n",
       "  'Scikit',\n",
       "  ' ',\n",
       "  'Matplotlib',\n",
       "  ' ',\n",
       "  'Plotly',\n",
       "  ' ',\n",
       "  'Docker',\n",
       "  ' ',\n",
       "  'GIT',\n",
       "  ' '],\n",
       " [' python ',\n",
       "  'name',\n",
       "  ' ',\n",
       "  'Rishiikeshwer Brindha Sivakumar',\n",
       "  ' ',\n",
       "  'key skills',\n",
       "  ' ',\n",
       "  'Python',\n",
       "  ' ',\n",
       "  'R',\n",
       "  ' ',\n",
       "  'C ',\n",
       "  ' ',\n",
       "  'C',\n",
       "  ' ',\n",
       "  'HTML',\n",
       "  ' ',\n",
       "  'PySpark',\n",
       "  ' ',\n",
       "  'SQL',\n",
       "  ' ',\n",
       "  'NoSQL',\n",
       "  ' ',\n",
       "  'Hadoop',\n",
       "  ' ',\n",
       "  'MongoDB',\n",
       "  ' ',\n",
       "  'PostgreSQL',\n",
       "  ' ',\n",
       "  'Hypothesis Testing',\n",
       "  ' ',\n",
       "  'Inferential Statistics',\n",
       "  ' ',\n",
       "  'A B testing',\n",
       "  ' ',\n",
       "  'EDA',\n",
       "  ' ',\n",
       "  'Regression',\n",
       "  ' ',\n",
       "  'Classification',\n",
       "  ' ',\n",
       "  'Clustering',\n",
       "  ' ',\n",
       "  'Time Series Forecasting',\n",
       "  ' ',\n",
       "  'NLP',\n",
       "  ' ',\n",
       "  'PCA',\n",
       "  ' ',\n",
       "  'AWS',\n",
       "  ' ',\n",
       "  'Power BI',\n",
       "  ' ',\n",
       "  'OpenVino',\n",
       "  ' ',\n",
       "  'Tensorflow',\n",
       "  ' ',\n",
       "  'Kubeflow',\n",
       "  ' ',\n",
       "  'Kafka',\n",
       "  ' ',\n",
       "  'Advanced Excel',\n",
       "  ' ',\n",
       "  'ETL',\n",
       "  ' ',\n",
       "  'Kubernetes',\n",
       "  ' ',\n",
       "  'RedHat OpenShift',\n",
       "  ' ',\n",
       "  'JIRA',\n",
       "  ' ',\n",
       "  'Shiny',\n",
       "  ' ',\n",
       "  'Keras',\n",
       "  ' ',\n",
       "  'Pytorch',\n",
       "  ' ',\n",
       "  'Pandas',\n",
       "  ' ',\n",
       "  'Numpy',\n",
       "  ' ',\n",
       "  'Analytical Solver',\n",
       "  ' ',\n",
       "  'Scikit',\n",
       "  ' ',\n",
       "  'Matplotlib',\n",
       "  ' ',\n",
       "  'Plotly',\n",
       "  ' ',\n",
       "  'Docker',\n",
       "  ' ',\n",
       "  'GIT',\n",
       "  ' ']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "900e408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_list=[]\n",
    "for i in new_list:\n",
    "        res = [ele for ele in i if ele.strip()]\n",
    "        cleared_list.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9221066d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' python ',\n",
       "  'name',\n",
       "  'Rishiikeshwer Brindha Sivakumar',\n",
       "  'key skills',\n",
       "  'Python',\n",
       "  'R',\n",
       "  'C ',\n",
       "  'C',\n",
       "  'HTML',\n",
       "  'PySpark',\n",
       "  'SQL',\n",
       "  'NoSQL',\n",
       "  'Hadoop',\n",
       "  'MongoDB',\n",
       "  'PostgreSQL',\n",
       "  'Hypothesis Testing',\n",
       "  'Inferential Statistics',\n",
       "  'A B testing',\n",
       "  'EDA',\n",
       "  'Regression',\n",
       "  'Classification',\n",
       "  'Clustering',\n",
       "  'Time Series Forecasting',\n",
       "  'NLP',\n",
       "  'PCA',\n",
       "  'AWS',\n",
       "  'Power BI',\n",
       "  'OpenVino',\n",
       "  'Tensorflow',\n",
       "  'Kubeflow',\n",
       "  'Kafka',\n",
       "  'Advanced Excel',\n",
       "  'ETL',\n",
       "  'Kubernetes',\n",
       "  'RedHat OpenShift',\n",
       "  'JIRA',\n",
       "  'Shiny',\n",
       "  'Keras',\n",
       "  'Pytorch',\n",
       "  'Pandas',\n",
       "  'Numpy',\n",
       "  'Analytical Solver',\n",
       "  'Scikit',\n",
       "  'Matplotlib',\n",
       "  'Plotly',\n",
       "  'Docker',\n",
       "  'GIT'],\n",
       " [' python ',\n",
       "  'name',\n",
       "  'Rishiikeshwer Brindha Sivakumar',\n",
       "  'key skills',\n",
       "  'Python',\n",
       "  'R',\n",
       "  'C ',\n",
       "  'C',\n",
       "  'HTML',\n",
       "  'PySpark',\n",
       "  'SQL',\n",
       "  'NoSQL',\n",
       "  'Hadoop',\n",
       "  'MongoDB',\n",
       "  'PostgreSQL',\n",
       "  'Hypothesis Testing',\n",
       "  'Inferential Statistics',\n",
       "  'A B testing',\n",
       "  'EDA',\n",
       "  'Regression',\n",
       "  'Classification',\n",
       "  'Clustering',\n",
       "  'Time Series Forecasting',\n",
       "  'NLP',\n",
       "  'PCA',\n",
       "  'AWS',\n",
       "  'Power BI',\n",
       "  'OpenVino',\n",
       "  'Tensorflow',\n",
       "  'Kubeflow',\n",
       "  'Kafka',\n",
       "  'Advanced Excel',\n",
       "  'ETL',\n",
       "  'Kubernetes',\n",
       "  'RedHat OpenShift',\n",
       "  'JIRA',\n",
       "  'Shiny',\n",
       "  'Keras',\n",
       "  'Pytorch',\n",
       "  'Pandas',\n",
       "  'Numpy',\n",
       "  'Analytical Solver',\n",
       "  'Scikit',\n",
       "  'Matplotlib',\n",
       "  'Plotly',\n",
       "  'Docker',\n",
       "  'GIT']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleared_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e7fc93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_name = 0\n",
    "to_find = \"name\"\n",
    "for i in cleared_list:\n",
    "    temp_lowercase_list = [item.lower() for item in i]\n",
    "    index_of_name = temp_lowercase_list.index(to_find.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e70e0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': ['Rishiikeshwer Brindha Sivakumar',\n",
       "  'Rishiikeshwer Brindha Sivakumar'],\n",
       " 'skills': [['Python',\n",
       "   'R',\n",
       "   'C ',\n",
       "   'C',\n",
       "   'HTML',\n",
       "   'PySpark',\n",
       "   'SQL',\n",
       "   'NoSQL',\n",
       "   'Hadoop',\n",
       "   'MongoDB',\n",
       "   'PostgreSQL',\n",
       "   'Hypothesis Testing',\n",
       "   'Inferential Statistics',\n",
       "   'A B testing',\n",
       "   'EDA',\n",
       "   'Regression',\n",
       "   'Classification',\n",
       "   'Clustering',\n",
       "   'Time Series Forecasting',\n",
       "   'NLP',\n",
       "   'PCA',\n",
       "   'AWS',\n",
       "   'Power BI',\n",
       "   'OpenVino',\n",
       "   'Tensorflow',\n",
       "   'Kubeflow',\n",
       "   'Kafka',\n",
       "   'Advanced Excel',\n",
       "   'ETL',\n",
       "   'Kubernetes',\n",
       "   'RedHat OpenShift',\n",
       "   'JIRA',\n",
       "   'Shiny',\n",
       "   'Keras',\n",
       "   'Pytorch',\n",
       "   'Pandas',\n",
       "   'Numpy',\n",
       "   'Analytical Solver',\n",
       "   'Scikit',\n",
       "   'Matplotlib',\n",
       "   'Plotly',\n",
       "   'Docker',\n",
       "   'GIT'],\n",
       "  ['Python',\n",
       "   'R',\n",
       "   'C ',\n",
       "   'C',\n",
       "   'HTML',\n",
       "   'PySpark',\n",
       "   'SQL',\n",
       "   'NoSQL',\n",
       "   'Hadoop',\n",
       "   'MongoDB',\n",
       "   'PostgreSQL',\n",
       "   'Hypothesis Testing',\n",
       "   'Inferential Statistics',\n",
       "   'A B testing',\n",
       "   'EDA',\n",
       "   'Regression',\n",
       "   'Classification',\n",
       "   'Clustering',\n",
       "   'Time Series Forecasting',\n",
       "   'NLP',\n",
       "   'PCA',\n",
       "   'AWS',\n",
       "   'Power BI',\n",
       "   'OpenVino',\n",
       "   'Tensorflow',\n",
       "   'Kubeflow',\n",
       "   'Kafka',\n",
       "   'Advanced Excel',\n",
       "   'ETL',\n",
       "   'Kubernetes',\n",
       "   'RedHat OpenShift',\n",
       "   'JIRA',\n",
       "   'Shiny',\n",
       "   'Keras',\n",
       "   'Pytorch',\n",
       "   'Pandas',\n",
       "   'Numpy',\n",
       "   'Analytical Solver',\n",
       "   'Scikit',\n",
       "   'Matplotlib',\n",
       "   'Plotly',\n",
       "   'Docker',\n",
       "   'GIT']]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_of_list = {\"Name\":[],\"skills\":[]};\n",
    "for i in cleared_list:\n",
    "    dictionary_of_list[\"Name\"].append(i[index_of_name+1])\n",
    "    dictionary_of_list[\"skills\"].append(i[index_of_name+3:])    \n",
    "dictionary_of_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ff32273",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rishiikeshwer Brindha Sivakumar</td>\n",
       "      <td>[Python, R, C , C, HTML, PySpark, SQL, NoSQL, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rishiikeshwer Brindha Sivakumar</td>\n",
       "      <td>[Python, R, C , C, HTML, PySpark, SQL, NoSQL, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name  \\\n",
       "0  Rishiikeshwer Brindha Sivakumar   \n",
       "1  Rishiikeshwer Brindha Sivakumar   \n",
       "\n",
       "                                              skills  \n",
       "0  [Python, R, C , C, HTML, PySpark, SQL, NoSQL, ...  \n",
       "1  [Python, R, C , C, HTML, PySpark, SQL, NoSQL, ...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "output_df = pd.DataFrame.from_dict(dictionary_of_list)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39f73720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract from JD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5eb7473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Prompt 1:\\n\\nDESCRIPTION\\nAmazon’s global fulfillment network enables any merchant to ship items that are ordered on Amazon to any place on earth. There is a complex network of ways in which items move between vendor locations, Amazon warehouses, and customer locations as well as several intermediate locations through which packages travel before reaching the customer. With a scale of millions of packages, each with different attributes and delivery requirements, what results is a highly dense graph of nodes.\\n\\nWe have built a highly respected software engineering team which is focused on solving complex problems in worldwide transportation using workflows, optimization algorithms, and machine learning systems. These are large-scale distributed systems handling millions of packages being shipped through the Amazon logistics network.\\n\\nYou will be working with senior engineers and principals to solve problems of scale, improve existing services & build new ones, and work on deep and complex algorithms to improve the experience of our customers globally while optimizing network operations.\\n\\nWe are open to hiring candidates to work out of one of the following locations:\\n\\nBellevue, WA, USA\\nBASIC QUALIFICATIONS\\n- 3+ years of data engineering experience\\n- 2+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience\\n- Knowledge of distributed systems as it pertains to data storage and computing\\n- Experience with data modeling, warehousing and building ETL pipelines\\n- Experience working on and delivering end to end projects independently\\n- Experience programming with at least one modern language such as C++, C#, Java, Python, Golang, PowerShell, Ruby\\n- Experience with Redshift, Oracle, NoSQL etc.\\nPREFERRED QUALIFICATIONS\\n- Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\\n- Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)\\n\\nPrompt 2:\\n\\nAmazon’s global fulfillment network enables any merchant to ship items that are ordered on Amazon to any place on earth. There is a complex network of ways in which items move between vendor locations, Amazon warehouses, and customer locations as well as several intermediate locations through which packages travel before reaching the customer. With a scale of millions of packages, each with different attributes and delivery requirements, what results is a highly dense graph of nodes.\\n\\nWe have built a highly respected software engineering team which is focused on solving complex problems in worldwide transportation using workflows, optimization algorithms, and machine learning systems. These are large-scale distributed systems handling millions of packages being shipped through the Amazon logistics network.\\n\\nYou will be working with senior engineers and principals to solve problems of scale, improve existing services & build new ones, and work on deep and complex algorithms to improve the experience of our customers globally while optimizing network operations.\\n\\nWe are open to hiring candidates to work out of one of the following locations:\\n\\nBellevue, WA, USA\\nBASIC QUALIFICATIONS\\n- 5+ years of data engineering experience\\n- Experience with data modeling, warehousing and building ETL pipelines\\n- Experience with SQL\\n- Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS\\n- Experience mentoring team members on best practices\\nPREFERRED QUALIFICATIONS\\n- Experience with big data technologies such as: Hadoop, Hive, Spark, EMR\\n- Experience operating large data warehouses\\n\\n\\nPrompt 3:\\n\\nJob Summary\\xa0\\nAt Walmart, we help people save money,\\u202fso they can live better. This mission serves as the foundation for every decision we make and drives us to create the future of retail. We\\xa0can’t\\xa0do that without the best talent – talent that is innovative, curious, and driven to create exceptional experiences for our customers.\\u202f\\u202f\\xa0\\xa0\\nDo you have boundless energy and passion for engineering data used to solve dynamic problems that will shape the future of retail?\\u202fWith the sheer scale of Walmart’s\\u202fenvironment\\u202fcomes the biggest of big data sets.\\u202fAs a\\u202fWalmart\\u202fData Engineer in Marketplace,\\u202fyou will dig\\u202finto our\\u202fmammoth\\u202fscale of\\u202fdata to help\\u202funleash\\u202fthe power of retail\\u202fdata science by\\u202fimagining, developing, and\\xa0maintaining\\xa0data pipelines\\u202fthat our Data Scientists and Analysts can rely on.\\u202f\\u202fYou will\\xa0be responsible for\\xa0contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way.\\u202fYou will\\u202fpartner\\u202fwith\\u202fData Scientists, Analysts, other\\xa0engineers\\u202fand business stakeholders to\\u202fsolve complex\\u202fand exciting\\u202fchallenges\\u202fso that\\u202fwe can\\u202fbuild out capabilities that\\u202fevolve the\\u202fretail business model\\u202fwhile\\u202fmaking\\u202fa\\u202fpositive\\u202fimpact\\u202fon\\u202four\\u202fcustomers’\\u202fand sellers’ lives.\\xa0\\n\\xa0\\nAbout the Data\\xa0and Customer Analytics (DCA)\\xa0Organization:\\xa0\\xa0\\nOur organization focuses on managing and delivering world-class data assets, including creating and\\xa0maintaining\\xa0data standards, driving policy compliance, creating partnerships, and developing pipelines and self-service tools. We empower our business to\\xa0leverage\\xa0data to fuel growth, driving revenue in our core and building new business model opportunities.\\u202f\\xa0\\xa0\\n\\xa0\\nWhat\\xa0you'll\\xa0do:\\u202f\\xa0\\xa0\\nYou will use\\xa0cutting edge\\xa0data engineering techniques to create critical datasets and dig into our mammoth scale of data to help unleash the power of data science by imagining, developing, and\\xa0maintaining\\xa0data pipelines that our Data Scientists and Analysts can rely on.\\xa0\\nYou will\\xa0be responsible for\\xa0contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed\\xa0way.\\xa0\\nYou will partner with Data Scientists, Analysts, other engineers, and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the marketplace business model while making a positive impact on our customers'\\xa0and sellers’\\xa0lives.\\xa0\\nYou will\\xa0participate\\xa0with limited help\\xa0in small to\\xa0large sized\\xa0projects by reviewing project requirements; gather requested information; write and develop code; conduct unit testing; communicate status and issues to team members and stakeholders; collaborate with project team and cross functional teams; troubleshoot open issues and bug-fixes; and ensure on-time delivery and hand-offs.\\xa0\\nYou will design,\\xa0develop\\xa0and\\xa0maintain\\xa0highly scalable and fault-tolerant real time, near real time and batch data systems/pipelines that process, store, and serve large volumes of data with\\xa0optimal\\xa0performance.\\xa0\\nYou will ensure data ingested and processed is\\xa0accurate\\xa0and of high quality by implementing data quality checks, data validation, and data cleaning processes.\\xa0\\nYou will\\xa0identify\\xa0possible options\\xa0to address business problems within one's discipline through analytics, big data analytics, and automation.\\xa0\\nYou will build business domain\\xa0knowledge\\xa0to support the\\xa0data need\\xa0for\\xa0product\\xa0teams,\\xa0analytics, data scientists and other data consumers.\\xa0\\n\\xa0\\nWhat\\xa0you'll\\xa0bring:\\xa0\\nBachelor's/master’s degree in computer science or a related field\\xa0\\nWith\\xa02+ years' experience in development of big data technologies/data pipelines\\u202f\\xa0\\nProficiency\\xa0in managing and manipulating huge datasets in the order of terabytes (TB) is\\xa0essential.\\u202f\\xa0\\nExpertise\\xa0in big data technologies like Hadoop, Apache Spark (Scala preferred), Apache Hive, or similar frameworks on the cloud (GCP preferred, AWS, Azure etc.) to build batch data pipelines with strong focus on optimization, SLA adherence and fault tolerance.\\u202f\\u202f\\xa0\\nExpertise\\xa0in building idempotent workflows using orchestrators like\\xa0Automic, Airflow, Luigi etc.\\u202f\\u202f\\xa0\\nExpertise\\xa0in writing SQL to analyze,\\xa0optimize, profile data preferably in\\xa0BigQuery\\xa0or SPARK SQL\\u202f\\xa0\\nStrong data modeling skills are necessary for designing a schema that can accommodate the evolution of data sources and\\xa0facilitate\\xa0seamless data joins across various\\xa0datasets.\\u202f\\xa0\\nAbility to work directly with stakeholders to understand data requirements and translate that to pipeline development / data solution\\xa0work.\\u202f\\xa0\\nStrong analytical and problem-solving skills are crucial for\\xa0identifying\\xa0and resolving issues that may arise during the data integration and schema evolution\\xa0process.\\u202f\\xa0\\nAbility\\xa0to move at\\xa0a\\xa0rapid pace with quality and start delivering with minimal ramp up time will be crucial to succeed in this\\xa0initiative.\\u202f\\xa0\\nEffective communication and collaboration skills are necessary for working in a team environment and coordinating efforts between different stakeholders involved in the\\xa0project.\\u202f\\xa0\\n\\xa0\\nNice to have from you:\\u202f\\xa0\\nExperience building complex near real time (NRT) streaming data pipelines using Apache Kafka, Spark streaming, Kafka Connect with a strong focus on stability,\\xa0scalability\\xa0and SLA adherence.\\u202f\\u202f\\xa0\\nGood understanding of REST APIs – working knowledge on Apache Druid, Redis, Elastic search,\\xa0GraphQL\\xa0or similar technologies.\\u202f Understanding of API contracts, building telemetry, stress testing etc.\\u202f\\xa0\\nExposure in developing reports/dashboards using Looker/Tableau\\u202f\\xa0\\nExperience in eCommerce domain.\\u202f\\u202f\\n\\nPrompt 4:\\n\\nRequires knowledge of Analytics/big data analytics / automation techniques and methods; Business understanding; Precedence and use cases; Business requirements and insights. To translate/ co-own business problems within one's discipline to data related or mathematical solutions. Identify appropriate methods/tools to be leveraged to provide a solution for the problem. Share use cases and gives examples to demonstrate how the method would solve the business problem\\nRequires knowledge of understanding of business value and relevance of data and data enabled insights / decisions; Appropriate application and understanding of data ecosystem including Data Management, Data Quality Standards and Data Governance, Accessibility, Storage and Scalability, etc.; Understanding of the methods and applications that unlock the monetary value of data assets. To Understand, articulate, interpret, and apply the principles of the defined strategy to unique, moderately complex business problems that may span one or main functions or domains\\nRequires knowledge of Data quality management techniques and standards; Business metadata definitions and content data definitions; Data profiling tools, data cleansing tools, data integration tools, and issues and event management tools; Understanding of user's data consumption, data needs, and business implications; Data modeling, storage, integration, and warehousing; Data quality framework and metrics; User access best practices; Enterprise data architecture, modeling and design, storage, integration, and warehousing; Enterprise data quality framework and metrics; Enterprise data strategy; Enterprise data quality strategy; Enterprise strategy to address regulatory and ethical requirements and policies around data privacy, security, storage, retention, and documentation. To promote and educate others on data quality awareness. Profile, analyze, and assess data quality. Test and validate data quality requirements. Continuously measure and monitor data quality. Deliver against data quality service level agreements. Manage operational Data Quality Management procedures. Manage data quality issues and leads data cleansing activities to remove data quality defects, improve data quality, and eliminate unused data. Determine user accessibility and removes or restricts user access as needed. Interpret company and regulatory policies on data. Educate others on data governance processes, practices, policies, and guidelines.\\nRequires knowledge of relevant Knowledge Discovery in Data (KDD) tools, applications, or scripting languages such as SQL, Oracle, Apache Mahout, MS Excel, Python; Statistical techniques (for example, mean, mode, median, variance, standard deviation, correlation, and sorting and grouping); Research analysis standards and activities; Documentation procedures such as drafting, editing, Bibliography format; Relevant Knowledge Discovery in Data (KDD) tools, applications, or scripting languages such as SQL, DB, SAS, Oracle, Apache Mahout, MS Excel, Python; KDD industry best practices and emerging trends. To collect and tabulate data and evaluate results to determine accuracy, validity, and applicability. Support the identification and application of statistical techniques based on requirements. Apply suitable technique under direction from leadership. Assist in the planning, design and implementation of an exploratory data analysis research projects. Understand existing statistical models and identify and recommend statistical models based on hypothesis. Use advanced Knowledge in Data Discovery tools to write queries and analyze data to identify patterns, trends, outliers, and correlations. Conduct statistical analysis (for example hypothesis tests, confidence intervals) and build basic statistical models using relevant packages/software suites.\\nDrives the execution of multiple business plans and projects by identifying customer and operational needs; developing and communicating business plans and priorities; removing barriers and obstacles that impact performance; providing resources; identifying performance standards; measuring progress and adjusting performance accordingly; developing contingency plans; and demonstrating adaptability and supporting continuous learning\\nData Source Identification: Requires knowledge of Functional business domain and scenarios; Categories of data and where it is held; Business data requirements; Database technologies and distributed datastores (e.g. SQL, NoSQL); Data Quality; Existing business systems and processes, including the key drivers and measures of success. To support the understanding of the priority order of requirements and service level agreements. Help identify the most suitable source for data that is fit for purpose. Perform initial data quality checks on extracted data.\\nUnderstanding Business Context: Requires knowledge of Industry and environmental factors; Common business vernacular; Business practices across two or more domains such as product, finance, marketing, sales, technology, business systems, and human resources and in-depth knowledge of related practices; Directly relevant business metrics and business areas. To Provide recommendations to business stakeholders to solve complex business issues. Develop business cases for projects with a projected return on investment or cost savings. Translate business requirements into projects, activities, and tasks and aligns to overall business strategy and develops domain specific artifact. Serve as an interpreter and conduit to connect business needs with tangible solutions and results. Identify and recommend relevant business insights pertaining to their area of work\\nMinimum Requirements\\nBachelor's degree in Business, Engineering, Statistics, Economics, Analytics, Mathematics, Arts, Finance or related field and 5 years' experience in data management, data science, or related field.\\nHands-on experience on cloud technologies – GCP, BQ, GCS, GSutil, automation tools – Airflow, Relational and non-relational data storage techniques – SQL, Spark, Hive, PySpark, Python, Jupiter notebook, Jupiterhub, command line functions to interact with GS, Github\\n\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "from docx import Document\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.lower().endswith('.pdf'):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.lower().endswith('.docx'):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        return \"\"  # Return empty string for unsupported file types\n",
    "\n",
    "# List of file types\n",
    "list_file_types = ('jd/*.docx', 'jd/*.pdf')\n",
    "\n",
    "# List to store extracted text\n",
    "job_description_text_list = []\n",
    "\n",
    "# Iterate over file types and extract text from each file\n",
    "for file_type in list_file_types:\n",
    "    files = glob.glob(file_type)\n",
    "    for file_path in files:\n",
    "        text = extract_text_from_file(file_path)\n",
    "        job_description_text_list.append(text)\n",
    "\n",
    "# Now you can use job_description_text_list for further processing\n",
    "#for text in job_description_text_list:\n",
    " #   print(text)\n",
    "job_description_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d224fc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT Response:\n",
      "```python\n",
      "key_skills = {\n",
      "    \"Prompt_1\": {\n",
      "        \"Basic_Qualifications\": [\n",
      "            \"Data engineering\",\n",
      "            \"Data analysis\",\n",
      "            \"Redshift\",\n",
      "            \"Oracle\",\n",
      "            \"NoSQL\",\n",
      "            \"Distributed systems\",\n",
      "            \"Data storage\",\n",
      "            \"Computing\",\n",
      "            \"Data modeling\",\n",
      "            \"Warehousing\",\n",
      "            \"ETL pipelines\",\n",
      "            \"Project delivery\",\n",
      "            \"Programming\",\n",
      "            \"C++\",\n",
      "            \"C#\",\n",
      "            \"Java\",\n",
      "            \"Python\",\n",
      "            \"Golang\",\n",
      "            \"PowerShell\",\n",
      "            \"Ruby\"\n",
      "        ],\n",
      "        \"Preferred_Qualifications\": [\n",
      "            \"AWS technologies\",\n",
      "            \"S3\",\n",
      "            \"AWS Glue\",\n",
      "            \"EMR\",\n",
      "            \"Kinesis\",\n",
      "            \"FireHose\",\n",
      "            \"Lambda\",\n",
      "            \"IAM roles and permissions\",\n",
      "            \"Non-relational databases\",\n",
      "            \"Object storage\",\n",
      "            \"Document stores\",\n",
      "            \"Key-value stores\",\n",
      "            \"Graph databases\",\n",
      "            \"Column-family databases\"\n",
      "        ]\n",
      "    },\n",
      "    \"Prompt_2\": {\n",
      "        \"Basic_Qualifications\": [\n",
      "            \"Data engineering\",\n",
      "            \"Data modeling\",\n",
      "            \"Warehousing\",\n",
      "            \"ETL pipelines\",\n",
      "            \"SQL\",\n",
      "            \"Scripting\",\n",
      "            \"Programming\",\n",
      "            \"Python\",\n",
      "            \"Java\",\n",
      "            \"Scala\",\n",
      "            \"NodeJS\",\n",
      "            \"Mentoring\"\n",
      "        ],\n",
      "        \"Preferred_Qualifications\": [\n",
      "            \"Big data technologies\",\n",
      "            \"Hadoop\",\n",
      "            \"Hive\",\n",
      "            \"Spark\",\n",
      "            \"EMR\",\n",
      "            \"Large data warehouses\"\n",
      "        ]\n",
      "    },\n",
      "    \"Prompt_3\": {\n",
      "        \"Basic_Qualifications\": [\n",
      "            \"Big data technologies\",\n",
      "            \"Data pipelines\",\n",
      "            \"Data transformations\",\n",
      "            \"Data quality checks\",\n",
      "            \"Data validation\",\n",
      "            \"Data cleaning\",\n",
      "            \"Data systems/pipelines\",\n",
      "            \"Data storage\",\n",
      "            \"Hadoop\",\n",
      "            \"Apache Spark\",\n",
      "            \"Apache Hive\",\n",
      "            \"Cloud frameworks\",\n",
      "            \"GCP\",\n",
      "            \"AWS\",\n",
      "            \"Azure\",\n",
      "            \"Orchestrators\",\n",
      "            \"Automic\",\n",
      "            \"Airflow\",\n",
      "            \"Luigi\",\n",
      "            \"SQL\",\n",
      "            \"BigQuery\",\n",
      "            \"SPARK SQL\",\n",
      "            \"Data modeling\",\n",
      "            \"Problem-solving\",\n",
      "            \"Communication\",\n",
      "            \"Collaboration\"\n",
      "        ],\n",
      "        \"Preferred_Qualifications\": [\n",
      "            \"Streaming data pipelines\",\n",
      "            \"Apache Kafka\",\n",
      "            \"Spark streaming\",\n",
      "            \"Kafka Connect\",\n",
      "            \"REST APIs\",\n",
      "            \"Apache Druid\",\n",
      "            \"Redis\",\n",
      "            \"Elastic search\",\n",
      "            \"GraphQL\",\n",
      "            \"Looker\",\n",
      "            \"Tableau\",\n",
      "            \"eCommerce domain\"\n",
      "        ]\n",
      "    },\n",
      "    \"Prompt_4\": {\n",
      "        \"Basic_Qualifications\": [\n",
      "            \"Analytics\",\n",
      "            \"Big data analytics\",\n",
      "            \"Automation\",\n",
      "            \"Data Management\",\n",
      "            \"Data Quality Standards\",\n",
      "            \"Data Governance\",\n",
      "            \"Data Storage\",\n",
      "            \"Scalability\",\n",
      "            \"Data quality management\",\n",
      "            \"Data profiling\",\n",
      "            \"Data cleansing\",\n",
      "            \"Data integration\",\n",
      "            \"Data modeling\",\n",
      "            \"Warehousing\",\n",
      "            \"Data strategy\",\n",
      "            \"KDD tools\",\n",
      "            \"SQL\",\n",
      "            \"Oracle\",\n",
      "            \"Apache Mahout\",\n",
      "            \"MS Excel\",\n",
      "            \"Python\",\n",
      "            \"Statistical techniques\",\n",
      "            \"Data Discovery\",\n",
      "            \"Data Source Identification\",\n",
      "            \"Database technologies\",\n",
      "            \"Business Context Understanding\",\n",
      "            \"Cloud technologies\",\n",
      "            \"GCP\",\n",
      "            \"BQ\",\n",
      "            \"GCS\",\n",
      "            \"GSutil\",\n",
      "            \"Airflow\",\n",
      "            \"Relational and non-relational data storage\",\n",
      "            \"Spark\",\n",
      "            \"Hive\",\n",
      "            \"PySpark\",\n",
      "            \"Jupyter notebook\",\n",
      "            \"Command line functions\",\n",
      "            \"Github\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['```python\\nkey_skills = {\\n    \"Prompt_1\": {\\n        \"Basic_Qualifications\": [\\n            \"Data engineering\",\\n            \"Data analysis\",\\n            \"Redshift\",\\n            \"Oracle\",\\n            \"NoSQL\",\\n            \"Distributed systems\",\\n            \"Data storage\",\\n            \"Computing\",\\n            \"Data modeling\",\\n            \"Warehousing\",\\n            \"ETL pipelines\",\\n            \"Project delivery\",\\n            \"Programming\",\\n            \"C++\",\\n            \"C#\",\\n            \"Java\",\\n            \"Python\",\\n            \"Golang\",\\n            \"PowerShell\",\\n            \"Ruby\"\\n        ],\\n        \"Preferred_Qualifications\": [\\n            \"AWS technologies\",\\n            \"S3\",\\n            \"AWS Glue\",\\n            \"EMR\",\\n            \"Kinesis\",\\n            \"FireHose\",\\n            \"Lambda\",\\n            \"IAM roles and permissions\",\\n            \"Non-relational databases\",\\n            \"Object storage\",\\n            \"Document stores\",\\n            \"Key-value stores\",\\n            \"Graph databases\",\\n            \"Column-family databases\"\\n        ]\\n    },\\n    \"Prompt_2\": {\\n        \"Basic_Qualifications\": [\\n            \"Data engineering\",\\n            \"Data modeling\",\\n            \"Warehousing\",\\n            \"ETL pipelines\",\\n            \"SQL\",\\n            \"Scripting\",\\n            \"Programming\",\\n            \"Python\",\\n            \"Java\",\\n            \"Scala\",\\n            \"NodeJS\",\\n            \"Mentoring\"\\n        ],\\n        \"Preferred_Qualifications\": [\\n            \"Big data technologies\",\\n            \"Hadoop\",\\n            \"Hive\",\\n            \"Spark\",\\n            \"EMR\",\\n            \"Large data warehouses\"\\n        ]\\n    },\\n    \"Prompt_3\": {\\n        \"Basic_Qualifications\": [\\n            \"Big data technologies\",\\n            \"Data pipelines\",\\n            \"Data transformations\",\\n            \"Data quality checks\",\\n            \"Data validation\",\\n            \"Data cleaning\",\\n            \"Data systems/pipelines\",\\n            \"Data storage\",\\n            \"Hadoop\",\\n            \"Apache Spark\",\\n            \"Apache Hive\",\\n            \"Cloud frameworks\",\\n            \"GCP\",\\n            \"AWS\",\\n            \"Azure\",\\n            \"Orchestrators\",\\n            \"Automic\",\\n            \"Airflow\",\\n            \"Luigi\",\\n            \"SQL\",\\n            \"BigQuery\",\\n            \"SPARK SQL\",\\n            \"Data modeling\",\\n            \"Problem-solving\",\\n            \"Communication\",\\n            \"Collaboration\"\\n        ],\\n        \"Preferred_Qualifications\": [\\n            \"Streaming data pipelines\",\\n            \"Apache Kafka\",\\n            \"Spark streaming\",\\n            \"Kafka Connect\",\\n            \"REST APIs\",\\n            \"Apache Druid\",\\n            \"Redis\",\\n            \"Elastic search\",\\n            \"GraphQL\",\\n            \"Looker\",\\n            \"Tableau\",\\n            \"eCommerce domain\"\\n        ]\\n    },\\n    \"Prompt_4\": {\\n        \"Basic_Qualifications\": [\\n            \"Analytics\",\\n            \"Big data analytics\",\\n            \"Automation\",\\n            \"Data Management\",\\n            \"Data Quality Standards\",\\n            \"Data Governance\",\\n            \"Data Storage\",\\n            \"Scalability\",\\n            \"Data quality management\",\\n            \"Data profiling\",\\n            \"Data cleansing\",\\n            \"Data integration\",\\n            \"Data modeling\",\\n            \"Warehousing\",\\n            \"Data strategy\",\\n            \"KDD tools\",\\n            \"SQL\",\\n            \"Oracle\",\\n            \"Apache Mahout\",\\n            \"MS Excel\",\\n            \"Python\",\\n            \"Statistical techniques\",\\n            \"Data Discovery\",\\n            \"Data Source Identification\",\\n            \"Database technologies\",\\n            \"Business Context Understanding\",\\n            \"Cloud technologies\",\\n            \"GCP\",\\n            \"BQ\",\\n            \"GCS\",\\n            \"GSutil\",\\n            \"Airflow\",\\n            \"Relational and non-relational data storage\",\\n            \"Spark\",\\n            \"Hive\",\\n            \"PySpark\",\\n            \"Jupyter notebook\",\\n            \"Command line functions\",\\n            \"Github\"\\n        ]\\n    }\\n}\\n```']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.api_key = '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' # OpenAI API key\n",
    "all_responses_jd = [] \n",
    "def generate_response(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "         messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        max_tokens=4000,  # Adjust as needed\n",
    "        temperature=0,  # Adjust as needed (higher values for more randomness)\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "custom = \"From the above resumes, extract all the key skills as keywords and return as a python dictionary\"\n",
    "\n",
    "for item_in_list in job_description_text_list:\n",
    "    prompt = custom + '\\n'+ item_in_list # here 'extracted_text' contains the text we want to send to ChatGPT\n",
    "    try:\n",
    "        chatgpt_response = generate_response(prompt)\n",
    "        print(\"ChatGPT Response:\")\n",
    "        print(chatgpt_response)\n",
    "    except Exception as e:    \n",
    "        print(f\"An error occurred: {e}\")\n",
    "    all_responses_jd.append(chatgpt_response)   \n",
    "\n",
    "all_responses_jd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27234d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['```python\\nkey_skills = {\\n    \"Prompt_1\": {\\n        \"Basic_Qualifications\": [\\n            \"Data engineering\",\\n            \"Data analysis\",\\n            \"Redshift\",\\n            \"Oracle\",\\n            \"NoSQL\",\\n            \"Distributed systems\",\\n            \"Data storage\",\\n            \"Computing\",\\n            \"Data modeling\",\\n            \"Warehousing\",\\n            \"ETL pipelines\",\\n            \"Project delivery\",\\n            \"Programming\",\\n            \"C++\",\\n            \"C#\",\\n            \"Java\",\\n            \"Python\",\\n            \"Golang\",\\n            \"PowerShell\",\\n            \"Ruby\"\\n        ],\\n        \"Preferred_Qualifications\": [\\n            \"AWS technologies\",\\n            \"S3\",\\n            \"AWS Glue\",\\n            \"EMR\",\\n            \"Kinesis\",\\n            \"FireHose\",\\n            \"Lambda\",\\n            \"IAM roles and permissions\",\\n            \"Non-relational databases\",\\n            \"Object storage\",\\n            \"Document stores\",\\n            \"Key-value stores\",\\n            \"Graph databases\",\\n            \"Column-family databases\"\\n        ]\\n    },\\n    \"Prompt_2\": {\\n        \"Basic_Qualifications\": [\\n            \"Data engineering\",\\n            \"Data modeling\",\\n            \"Warehousing\",\\n            \"ETL pipelines\",\\n            \"SQL\",\\n            \"Scripting\",\\n            \"Programming\",\\n            \"Python\",\\n            \"Java\",\\n            \"Scala\",\\n            \"NodeJS\",\\n            \"Mentoring\"\\n        ],\\n        \"Preferred_Qualifications\": [\\n            \"Big data technologies\",\\n            \"Hadoop\",\\n            \"Hive\",\\n            \"Spark\",\\n            \"EMR\",\\n            \"Large data warehouses\"\\n        ]\\n    },\\n    \"Prompt_3\": {\\n        \"Basic_Qualifications\": [\\n            \"Big data technologies\",\\n            \"Data pipelines\",\\n            \"Data transformations\",\\n            \"Data quality checks\",\\n            \"Data validation\",\\n            \"Data cleaning\",\\n            \"Data systems/pipelines\",\\n            \"Data storage\",\\n            \"Hadoop\",\\n            \"Apache Spark\",\\n            \"Apache Hive\",\\n            \"Cloud frameworks\",\\n            \"GCP\",\\n            \"AWS\",\\n            \"Azure\",\\n            \"Orchestrators\",\\n            \"Automic\",\\n            \"Airflow\",\\n            \"Luigi\",\\n            \"SQL\",\\n            \"BigQuery\",\\n            \"SPARK SQL\",\\n            \"Data modeling\",\\n            \"Problem-solving\",\\n            \"Communication\",\\n            \"Collaboration\"\\n        ],\\n        \"Preferred_Qualifications\": [\\n            \"Streaming data pipelines\",\\n            \"Apache Kafka\",\\n            \"Spark streaming\",\\n            \"Kafka Connect\",\\n            \"REST APIs\",\\n            \"Apache Druid\",\\n            \"Redis\",\\n            \"Elastic search\",\\n            \"GraphQL\",\\n            \"Looker\",\\n            \"Tableau\",\\n            \"eCommerce domain\"\\n        ]\\n    },\\n    \"Prompt_4\": {\\n        \"Basic_Qualifications\": [\\n            \"Analytics\",\\n            \"Big data analytics\",\\n            \"Automation\",\\n            \"Data Management\",\\n            \"Data Quality Standards\",\\n            \"Data Governance\",\\n            \"Data Storage\",\\n            \"Scalability\",\\n            \"Data quality management\",\\n            \"Data profiling\",\\n            \"Data cleansing\",\\n            \"Data integration\",\\n            \"Data modeling\",\\n            \"Warehousing\",\\n            \"Data strategy\",\\n            \"KDD tools\",\\n            \"SQL\",\\n            \"Oracle\",\\n            \"Apache Mahout\",\\n            \"MS Excel\",\\n            \"Python\",\\n            \"Statistical techniques\",\\n            \"Data Discovery\",\\n            \"Data Source Identification\",\\n            \"Database technologies\",\\n            \"Business Context Understanding\",\\n            \"Cloud technologies\",\\n            \"GCP\",\\n            \"BQ\",\\n            \"GCS\",\\n            \"GSutil\",\\n            \"Airflow\",\\n            \"Relational and non-relational data storage\",\\n            \"Spark\",\\n            \"Hive\",\\n            \"PySpark\",\\n            \"Jupyter notebook\",\\n            \"Command line functions\",\\n            \"Github\"\\n        ]\\n    }\\n}\\n```']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_responses_jd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a80d15fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(all_responses_jd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33ffc7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses_jd_split=[]\n",
    "for i in all_responses_jd:\n",
    "    each_iteration = i.split('\"')\n",
    "    all_responses_jd_split.append(each_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d35b1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['```python\\nkey_skills = {\\n    ', 'Prompt_1', ': {\\n        ', 'Basic_Qualifications', ': [\\n            ', 'Data engineering', ',\\n            ', 'Data analysis', ',\\n            ', 'Redshift', ',\\n            ', 'Oracle', ',\\n            ', 'NoSQL', ',\\n            ', 'Distributed systems', ',\\n            ', 'Data storage', ',\\n            ', 'Computing', ',\\n            ', 'Data modeling', ',\\n            ', 'Warehousing', ',\\n            ', 'ETL pipelines', ',\\n            ', 'Project delivery', ',\\n            ', 'Programming', ',\\n            ', 'C++', ',\\n            ', 'C#', ',\\n            ', 'Java', ',\\n            ', 'Python', ',\\n            ', 'Golang', ',\\n            ', 'PowerShell', ',\\n            ', 'Ruby', '\\n        ],\\n        ', 'Preferred_Qualifications', ': [\\n            ', 'AWS technologies', ',\\n            ', 'S3', ',\\n            ', 'AWS Glue', ',\\n            ', 'EMR', ',\\n            ', 'Kinesis', ',\\n            ', 'FireHose', ',\\n            ', 'Lambda', ',\\n            ', 'IAM roles and permissions', ',\\n            ', 'Non-relational databases', ',\\n            ', 'Object storage', ',\\n            ', 'Document stores', ',\\n            ', 'Key-value stores', ',\\n            ', 'Graph databases', ',\\n            ', 'Column-family databases', '\\n        ]\\n    },\\n    ', 'Prompt_2', ': {\\n        ', 'Basic_Qualifications', ': [\\n            ', 'Data engineering', ',\\n            ', 'Data modeling', ',\\n            ', 'Warehousing', ',\\n            ', 'ETL pipelines', ',\\n            ', 'SQL', ',\\n            ', 'Scripting', ',\\n            ', 'Programming', ',\\n            ', 'Python', ',\\n            ', 'Java', ',\\n            ', 'Scala', ',\\n            ', 'NodeJS', ',\\n            ', 'Mentoring', '\\n        ],\\n        ', 'Preferred_Qualifications', ': [\\n            ', 'Big data technologies', ',\\n            ', 'Hadoop', ',\\n            ', 'Hive', ',\\n            ', 'Spark', ',\\n            ', 'EMR', ',\\n            ', 'Large data warehouses', '\\n        ]\\n    },\\n    ', 'Prompt_3', ': {\\n        ', 'Basic_Qualifications', ': [\\n            ', 'Big data technologies', ',\\n            ', 'Data pipelines', ',\\n            ', 'Data transformations', ',\\n            ', 'Data quality checks', ',\\n            ', 'Data validation', ',\\n            ', 'Data cleaning', ',\\n            ', 'Data systems/pipelines', ',\\n            ', 'Data storage', ',\\n            ', 'Hadoop', ',\\n            ', 'Apache Spark', ',\\n            ', 'Apache Hive', ',\\n            ', 'Cloud frameworks', ',\\n            ', 'GCP', ',\\n            ', 'AWS', ',\\n            ', 'Azure', ',\\n            ', 'Orchestrators', ',\\n            ', 'Automic', ',\\n            ', 'Airflow', ',\\n            ', 'Luigi', ',\\n            ', 'SQL', ',\\n            ', 'BigQuery', ',\\n            ', 'SPARK SQL', ',\\n            ', 'Data modeling', ',\\n            ', 'Problem-solving', ',\\n            ', 'Communication', ',\\n            ', 'Collaboration', '\\n        ],\\n        ', 'Preferred_Qualifications', ': [\\n            ', 'Streaming data pipelines', ',\\n            ', 'Apache Kafka', ',\\n            ', 'Spark streaming', ',\\n            ', 'Kafka Connect', ',\\n            ', 'REST APIs', ',\\n            ', 'Apache Druid', ',\\n            ', 'Redis', ',\\n            ', 'Elastic search', ',\\n            ', 'GraphQL', ',\\n            ', 'Looker', ',\\n            ', 'Tableau', ',\\n            ', 'eCommerce domain', '\\n        ]\\n    },\\n    ', 'Prompt_4', ': {\\n        ', 'Basic_Qualifications', ': [\\n            ', 'Analytics', ',\\n            ', 'Big data analytics', ',\\n            ', 'Automation', ',\\n            ', 'Data Management', ',\\n            ', 'Data Quality Standards', ',\\n            ', 'Data Governance', ',\\n            ', 'Data Storage', ',\\n            ', 'Scalability', ',\\n            ', 'Data quality management', ',\\n            ', 'Data profiling', ',\\n            ', 'Data cleansing', ',\\n            ', 'Data integration', ',\\n            ', 'Data modeling', ',\\n            ', 'Warehousing', ',\\n            ', 'Data strategy', ',\\n            ', 'KDD tools', ',\\n            ', 'SQL', ',\\n            ', 'Oracle', ',\\n            ', 'Apache Mahout', ',\\n            ', 'MS Excel', ',\\n            ', 'Python', ',\\n            ', 'Statistical techniques', ',\\n            ', 'Data Discovery', ',\\n            ', 'Data Source Identification', ',\\n            ', 'Database technologies', ',\\n            ', 'Business Context Understanding', ',\\n            ', 'Cloud technologies', ',\\n            ', 'GCP', ',\\n            ', 'BQ', ',\\n            ', 'GCS', ',\\n            ', 'GSutil', ',\\n            ', 'Airflow', ',\\n            ', 'Relational and non-relational data storage', ',\\n            ', 'Spark', ',\\n            ', 'Hive', ',\\n            ', 'PySpark', ',\\n            ', 'Jupyter notebook', ',\\n            ', 'Command line functions', ',\\n            ', 'Github', '\\n        ]\\n    }\\n}\\n```']]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(all_responses_jd_split)\n",
    "print(type(all_responses_jd_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31622a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "new_list_jd = []\n",
    "\n",
    "for i in all_responses_jd_split:\n",
    "    temp_list_jd =[]\n",
    "    for j in i:\n",
    "        remove_special_chars_jd = re.sub('[^A-Za-z0-9]+',' ', j)\n",
    "        temp_list_jd.append(remove_special_chars_jd)\n",
    "    new_list_jd.append(temp_list_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c21256f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' python key skills ',\n",
       "  'Prompt 1',\n",
       "  ' ',\n",
       "  'Basic Qualifications',\n",
       "  ' ',\n",
       "  'Data engineering',\n",
       "  ' ',\n",
       "  'Data analysis',\n",
       "  ' ',\n",
       "  'Redshift',\n",
       "  ' ',\n",
       "  'Oracle',\n",
       "  ' ',\n",
       "  'NoSQL',\n",
       "  ' ',\n",
       "  'Distributed systems',\n",
       "  ' ',\n",
       "  'Data storage',\n",
       "  ' ',\n",
       "  'Computing',\n",
       "  ' ',\n",
       "  'Data modeling',\n",
       "  ' ',\n",
       "  'Warehousing',\n",
       "  ' ',\n",
       "  'ETL pipelines',\n",
       "  ' ',\n",
       "  'Project delivery',\n",
       "  ' ',\n",
       "  'Programming',\n",
       "  ' ',\n",
       "  'C ',\n",
       "  ' ',\n",
       "  'C ',\n",
       "  ' ',\n",
       "  'Java',\n",
       "  ' ',\n",
       "  'Python',\n",
       "  ' ',\n",
       "  'Golang',\n",
       "  ' ',\n",
       "  'PowerShell',\n",
       "  ' ',\n",
       "  'Ruby',\n",
       "  ' ',\n",
       "  'Preferred Qualifications',\n",
       "  ' ',\n",
       "  'AWS technologies',\n",
       "  ' ',\n",
       "  'S3',\n",
       "  ' ',\n",
       "  'AWS Glue',\n",
       "  ' ',\n",
       "  'EMR',\n",
       "  ' ',\n",
       "  'Kinesis',\n",
       "  ' ',\n",
       "  'FireHose',\n",
       "  ' ',\n",
       "  'Lambda',\n",
       "  ' ',\n",
       "  'IAM roles and permissions',\n",
       "  ' ',\n",
       "  'Non relational databases',\n",
       "  ' ',\n",
       "  'Object storage',\n",
       "  ' ',\n",
       "  'Document stores',\n",
       "  ' ',\n",
       "  'Key value stores',\n",
       "  ' ',\n",
       "  'Graph databases',\n",
       "  ' ',\n",
       "  'Column family databases',\n",
       "  ' ',\n",
       "  'Prompt 2',\n",
       "  ' ',\n",
       "  'Basic Qualifications',\n",
       "  ' ',\n",
       "  'Data engineering',\n",
       "  ' ',\n",
       "  'Data modeling',\n",
       "  ' ',\n",
       "  'Warehousing',\n",
       "  ' ',\n",
       "  'ETL pipelines',\n",
       "  ' ',\n",
       "  'SQL',\n",
       "  ' ',\n",
       "  'Scripting',\n",
       "  ' ',\n",
       "  'Programming',\n",
       "  ' ',\n",
       "  'Python',\n",
       "  ' ',\n",
       "  'Java',\n",
       "  ' ',\n",
       "  'Scala',\n",
       "  ' ',\n",
       "  'NodeJS',\n",
       "  ' ',\n",
       "  'Mentoring',\n",
       "  ' ',\n",
       "  'Preferred Qualifications',\n",
       "  ' ',\n",
       "  'Big data technologies',\n",
       "  ' ',\n",
       "  'Hadoop',\n",
       "  ' ',\n",
       "  'Hive',\n",
       "  ' ',\n",
       "  'Spark',\n",
       "  ' ',\n",
       "  'EMR',\n",
       "  ' ',\n",
       "  'Large data warehouses',\n",
       "  ' ',\n",
       "  'Prompt 3',\n",
       "  ' ',\n",
       "  'Basic Qualifications',\n",
       "  ' ',\n",
       "  'Big data technologies',\n",
       "  ' ',\n",
       "  'Data pipelines',\n",
       "  ' ',\n",
       "  'Data transformations',\n",
       "  ' ',\n",
       "  'Data quality checks',\n",
       "  ' ',\n",
       "  'Data validation',\n",
       "  ' ',\n",
       "  'Data cleaning',\n",
       "  ' ',\n",
       "  'Data systems pipelines',\n",
       "  ' ',\n",
       "  'Data storage',\n",
       "  ' ',\n",
       "  'Hadoop',\n",
       "  ' ',\n",
       "  'Apache Spark',\n",
       "  ' ',\n",
       "  'Apache Hive',\n",
       "  ' ',\n",
       "  'Cloud frameworks',\n",
       "  ' ',\n",
       "  'GCP',\n",
       "  ' ',\n",
       "  'AWS',\n",
       "  ' ',\n",
       "  'Azure',\n",
       "  ' ',\n",
       "  'Orchestrators',\n",
       "  ' ',\n",
       "  'Automic',\n",
       "  ' ',\n",
       "  'Airflow',\n",
       "  ' ',\n",
       "  'Luigi',\n",
       "  ' ',\n",
       "  'SQL',\n",
       "  ' ',\n",
       "  'BigQuery',\n",
       "  ' ',\n",
       "  'SPARK SQL',\n",
       "  ' ',\n",
       "  'Data modeling',\n",
       "  ' ',\n",
       "  'Problem solving',\n",
       "  ' ',\n",
       "  'Communication',\n",
       "  ' ',\n",
       "  'Collaboration',\n",
       "  ' ',\n",
       "  'Preferred Qualifications',\n",
       "  ' ',\n",
       "  'Streaming data pipelines',\n",
       "  ' ',\n",
       "  'Apache Kafka',\n",
       "  ' ',\n",
       "  'Spark streaming',\n",
       "  ' ',\n",
       "  'Kafka Connect',\n",
       "  ' ',\n",
       "  'REST APIs',\n",
       "  ' ',\n",
       "  'Apache Druid',\n",
       "  ' ',\n",
       "  'Redis',\n",
       "  ' ',\n",
       "  'Elastic search',\n",
       "  ' ',\n",
       "  'GraphQL',\n",
       "  ' ',\n",
       "  'Looker',\n",
       "  ' ',\n",
       "  'Tableau',\n",
       "  ' ',\n",
       "  'eCommerce domain',\n",
       "  ' ',\n",
       "  'Prompt 4',\n",
       "  ' ',\n",
       "  'Basic Qualifications',\n",
       "  ' ',\n",
       "  'Analytics',\n",
       "  ' ',\n",
       "  'Big data analytics',\n",
       "  ' ',\n",
       "  'Automation',\n",
       "  ' ',\n",
       "  'Data Management',\n",
       "  ' ',\n",
       "  'Data Quality Standards',\n",
       "  ' ',\n",
       "  'Data Governance',\n",
       "  ' ',\n",
       "  'Data Storage',\n",
       "  ' ',\n",
       "  'Scalability',\n",
       "  ' ',\n",
       "  'Data quality management',\n",
       "  ' ',\n",
       "  'Data profiling',\n",
       "  ' ',\n",
       "  'Data cleansing',\n",
       "  ' ',\n",
       "  'Data integration',\n",
       "  ' ',\n",
       "  'Data modeling',\n",
       "  ' ',\n",
       "  'Warehousing',\n",
       "  ' ',\n",
       "  'Data strategy',\n",
       "  ' ',\n",
       "  'KDD tools',\n",
       "  ' ',\n",
       "  'SQL',\n",
       "  ' ',\n",
       "  'Oracle',\n",
       "  ' ',\n",
       "  'Apache Mahout',\n",
       "  ' ',\n",
       "  'MS Excel',\n",
       "  ' ',\n",
       "  'Python',\n",
       "  ' ',\n",
       "  'Statistical techniques',\n",
       "  ' ',\n",
       "  'Data Discovery',\n",
       "  ' ',\n",
       "  'Data Source Identification',\n",
       "  ' ',\n",
       "  'Database technologies',\n",
       "  ' ',\n",
       "  'Business Context Understanding',\n",
       "  ' ',\n",
       "  'Cloud technologies',\n",
       "  ' ',\n",
       "  'GCP',\n",
       "  ' ',\n",
       "  'BQ',\n",
       "  ' ',\n",
       "  'GCS',\n",
       "  ' ',\n",
       "  'GSutil',\n",
       "  ' ',\n",
       "  'Airflow',\n",
       "  ' ',\n",
       "  'Relational and non relational data storage',\n",
       "  ' ',\n",
       "  'Spark',\n",
       "  ' ',\n",
       "  'Hive',\n",
       "  ' ',\n",
       "  'PySpark',\n",
       "  ' ',\n",
       "  'Jupyter notebook',\n",
       "  ' ',\n",
       "  'Command line functions',\n",
       "  ' ',\n",
       "  'Github',\n",
       "  ' ']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list_jd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36333423",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_list_jd=[]\n",
    "for i in new_list_jd:\n",
    "        res = [ele for ele in i if ele.strip()]\n",
    "        cleared_list_jd.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d9d07ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' python key skills ',\n",
       "  'Prompt 1',\n",
       "  'Basic Qualifications',\n",
       "  'Data engineering',\n",
       "  'Data analysis',\n",
       "  'Redshift',\n",
       "  'Oracle',\n",
       "  'NoSQL',\n",
       "  'Distributed systems',\n",
       "  'Data storage',\n",
       "  'Computing',\n",
       "  'Data modeling',\n",
       "  'Warehousing',\n",
       "  'ETL pipelines',\n",
       "  'Project delivery',\n",
       "  'Programming',\n",
       "  'C ',\n",
       "  'C ',\n",
       "  'Java',\n",
       "  'Python',\n",
       "  'Golang',\n",
       "  'PowerShell',\n",
       "  'Ruby',\n",
       "  'Preferred Qualifications',\n",
       "  'AWS technologies',\n",
       "  'S3',\n",
       "  'AWS Glue',\n",
       "  'EMR',\n",
       "  'Kinesis',\n",
       "  'FireHose',\n",
       "  'Lambda',\n",
       "  'IAM roles and permissions',\n",
       "  'Non relational databases',\n",
       "  'Object storage',\n",
       "  'Document stores',\n",
       "  'Key value stores',\n",
       "  'Graph databases',\n",
       "  'Column family databases',\n",
       "  'Prompt 2',\n",
       "  'Basic Qualifications',\n",
       "  'Data engineering',\n",
       "  'Data modeling',\n",
       "  'Warehousing',\n",
       "  'ETL pipelines',\n",
       "  'SQL',\n",
       "  'Scripting',\n",
       "  'Programming',\n",
       "  'Python',\n",
       "  'Java',\n",
       "  'Scala',\n",
       "  'NodeJS',\n",
       "  'Mentoring',\n",
       "  'Preferred Qualifications',\n",
       "  'Big data technologies',\n",
       "  'Hadoop',\n",
       "  'Hive',\n",
       "  'Spark',\n",
       "  'EMR',\n",
       "  'Large data warehouses',\n",
       "  'Prompt 3',\n",
       "  'Basic Qualifications',\n",
       "  'Big data technologies',\n",
       "  'Data pipelines',\n",
       "  'Data transformations',\n",
       "  'Data quality checks',\n",
       "  'Data validation',\n",
       "  'Data cleaning',\n",
       "  'Data systems pipelines',\n",
       "  'Data storage',\n",
       "  'Hadoop',\n",
       "  'Apache Spark',\n",
       "  'Apache Hive',\n",
       "  'Cloud frameworks',\n",
       "  'GCP',\n",
       "  'AWS',\n",
       "  'Azure',\n",
       "  'Orchestrators',\n",
       "  'Automic',\n",
       "  'Airflow',\n",
       "  'Luigi',\n",
       "  'SQL',\n",
       "  'BigQuery',\n",
       "  'SPARK SQL',\n",
       "  'Data modeling',\n",
       "  'Problem solving',\n",
       "  'Communication',\n",
       "  'Collaboration',\n",
       "  'Preferred Qualifications',\n",
       "  'Streaming data pipelines',\n",
       "  'Apache Kafka',\n",
       "  'Spark streaming',\n",
       "  'Kafka Connect',\n",
       "  'REST APIs',\n",
       "  'Apache Druid',\n",
       "  'Redis',\n",
       "  'Elastic search',\n",
       "  'GraphQL',\n",
       "  'Looker',\n",
       "  'Tableau',\n",
       "  'eCommerce domain',\n",
       "  'Prompt 4',\n",
       "  'Basic Qualifications',\n",
       "  'Analytics',\n",
       "  'Big data analytics',\n",
       "  'Automation',\n",
       "  'Data Management',\n",
       "  'Data Quality Standards',\n",
       "  'Data Governance',\n",
       "  'Data Storage',\n",
       "  'Scalability',\n",
       "  'Data quality management',\n",
       "  'Data profiling',\n",
       "  'Data cleansing',\n",
       "  'Data integration',\n",
       "  'Data modeling',\n",
       "  'Warehousing',\n",
       "  'Data strategy',\n",
       "  'KDD tools',\n",
       "  'SQL',\n",
       "  'Oracle',\n",
       "  'Apache Mahout',\n",
       "  'MS Excel',\n",
       "  'Python',\n",
       "  'Statistical techniques',\n",
       "  'Data Discovery',\n",
       "  'Data Source Identification',\n",
       "  'Database technologies',\n",
       "  'Business Context Understanding',\n",
       "  'Cloud technologies',\n",
       "  'GCP',\n",
       "  'BQ',\n",
       "  'GCS',\n",
       "  'GSutil',\n",
       "  'Airflow',\n",
       "  'Relational and non relational data storage',\n",
       "  'Spark',\n",
       "  'Hive',\n",
       "  'PySpark',\n",
       "  'Jupyter notebook',\n",
       "  'Command line functions',\n",
       "  'Github']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleared_list_jd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c43c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index_of_name = 0\n",
    "#to_find = \"name\"\n",
    "for i in cleared_list_jd:\n",
    "    temp_lowercase_list_jd = [item.lower() for item in i]\n",
    "    #index_of_name = temp_lowercase_list.index(to_find.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0ababb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skills': [['Data analysis',\n",
       "   'Redshift',\n",
       "   'Oracle',\n",
       "   'NoSQL',\n",
       "   'Distributed systems',\n",
       "   'Data storage',\n",
       "   'Computing',\n",
       "   'Data modeling',\n",
       "   'Warehousing',\n",
       "   'ETL pipelines',\n",
       "   'Project delivery',\n",
       "   'Programming',\n",
       "   'C ',\n",
       "   'C ',\n",
       "   'Java',\n",
       "   'Python',\n",
       "   'Golang',\n",
       "   'PowerShell',\n",
       "   'Ruby',\n",
       "   'Preferred Qualifications',\n",
       "   'AWS technologies',\n",
       "   'S3',\n",
       "   'AWS Glue',\n",
       "   'EMR',\n",
       "   'Kinesis',\n",
       "   'FireHose',\n",
       "   'Lambda',\n",
       "   'IAM roles and permissions',\n",
       "   'Non relational databases',\n",
       "   'Object storage',\n",
       "   'Document stores',\n",
       "   'Key value stores',\n",
       "   'Graph databases',\n",
       "   'Column family databases',\n",
       "   'Prompt 2',\n",
       "   'Basic Qualifications',\n",
       "   'Data engineering',\n",
       "   'Data modeling',\n",
       "   'Warehousing',\n",
       "   'ETL pipelines',\n",
       "   'SQL',\n",
       "   'Scripting',\n",
       "   'Programming',\n",
       "   'Python',\n",
       "   'Java',\n",
       "   'Scala',\n",
       "   'NodeJS',\n",
       "   'Mentoring',\n",
       "   'Preferred Qualifications',\n",
       "   'Big data technologies',\n",
       "   'Hadoop',\n",
       "   'Hive',\n",
       "   'Spark',\n",
       "   'EMR',\n",
       "   'Large data warehouses',\n",
       "   'Prompt 3',\n",
       "   'Basic Qualifications',\n",
       "   'Big data technologies',\n",
       "   'Data pipelines',\n",
       "   'Data transformations',\n",
       "   'Data quality checks',\n",
       "   'Data validation',\n",
       "   'Data cleaning',\n",
       "   'Data systems pipelines',\n",
       "   'Data storage',\n",
       "   'Hadoop',\n",
       "   'Apache Spark',\n",
       "   'Apache Hive',\n",
       "   'Cloud frameworks',\n",
       "   'GCP',\n",
       "   'AWS',\n",
       "   'Azure',\n",
       "   'Orchestrators',\n",
       "   'Automic',\n",
       "   'Airflow',\n",
       "   'Luigi',\n",
       "   'SQL',\n",
       "   'BigQuery',\n",
       "   'SPARK SQL',\n",
       "   'Data modeling',\n",
       "   'Problem solving',\n",
       "   'Communication',\n",
       "   'Collaboration',\n",
       "   'Preferred Qualifications',\n",
       "   'Streaming data pipelines',\n",
       "   'Apache Kafka',\n",
       "   'Spark streaming',\n",
       "   'Kafka Connect',\n",
       "   'REST APIs',\n",
       "   'Apache Druid',\n",
       "   'Redis',\n",
       "   'Elastic search',\n",
       "   'GraphQL',\n",
       "   'Looker',\n",
       "   'Tableau',\n",
       "   'eCommerce domain',\n",
       "   'Prompt 4',\n",
       "   'Basic Qualifications',\n",
       "   'Analytics',\n",
       "   'Big data analytics',\n",
       "   'Automation',\n",
       "   'Data Management',\n",
       "   'Data Quality Standards',\n",
       "   'Data Governance',\n",
       "   'Data Storage',\n",
       "   'Scalability',\n",
       "   'Data quality management',\n",
       "   'Data profiling',\n",
       "   'Data cleansing',\n",
       "   'Data integration',\n",
       "   'Data modeling',\n",
       "   'Warehousing',\n",
       "   'Data strategy',\n",
       "   'KDD tools',\n",
       "   'SQL',\n",
       "   'Oracle',\n",
       "   'Apache Mahout',\n",
       "   'MS Excel',\n",
       "   'Python',\n",
       "   'Statistical techniques',\n",
       "   'Data Discovery',\n",
       "   'Data Source Identification',\n",
       "   'Database technologies',\n",
       "   'Business Context Understanding',\n",
       "   'Cloud technologies',\n",
       "   'GCP',\n",
       "   'BQ',\n",
       "   'GCS',\n",
       "   'GSutil',\n",
       "   'Airflow',\n",
       "   'Relational and non relational data storage',\n",
       "   'Spark',\n",
       "   'Hive',\n",
       "   'PySpark',\n",
       "   'Jupyter notebook',\n",
       "   'Command line functions',\n",
       "   'Github']]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_of_list_jd = {\"skills\":[]};\n",
    "for i in cleared_list_jd:\n",
    "    #dictionary_of_list[\"Name\"].append(i[index_of_name+1])\n",
    "    dictionary_of_list_jd[\"skills\"].append(i[index_of_name+3:])    \n",
    "dictionary_of_list_jd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0fc44cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Data analysis, Redshift, Oracle, NoSQL, Distr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              skills\n",
       "0  [Data analysis, Redshift, Oracle, NoSQL, Distr..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "output_df_jd = pd.DataFrame.from_dict(dictionary_of_list_jd)\n",
    "output_df_jd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c93fbc23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1\n",
      "0  0.101322  0.101322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming output_df_jd and output_df are already defined dataframes\n",
    "\n",
    "# Convert keyword lists to sets\n",
    "output_df_jd['skills'] = output_df_jd['skills'].apply(set)\n",
    "output_df['skills'] = output_df['skills'].apply(set)\n",
    "\n",
    "# Create a set of all unique keywords\n",
    "all_keywords = set().union(*output_df_jd['skills'], *output_df['skills'])\n",
    "\n",
    "# Create vectors for job description and resumes\n",
    "vector_jd = output_df_jd['skills'].apply(lambda x: [1 if keyword in x else 0 for keyword in all_keywords])\n",
    "vector_resume = output_df['skills'].apply(lambda x: [1 if keyword in x else 0 for keyword in all_keywords])\n",
    "\n",
    "# Convert vectors to dataframe\n",
    "vector_df_jd = pd.DataFrame(vector_jd.tolist(), columns=all_keywords)\n",
    "vector_df_resume = pd.DataFrame(vector_resume.tolist(), columns=all_keywords)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cos_sim = cosine_similarity(vector_df_jd, vector_df_resume)\n",
    "\n",
    "# Convert cosine similarity to DataFrame\n",
    "cos_sim_df = pd.DataFrame(cos_sim, index=output_df_jd.index, columns=output_df.index)\n",
    "\n",
    "# Display the resulting cosine similarity DataFrame\n",
    "print(cos_sim_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d49fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d5ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec610d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
